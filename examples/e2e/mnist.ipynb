{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3bb75c-8160-442c-aed0-05849b37a9fd",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "#### Loading MNIST code taken from https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/dataset/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8d38b4-f3b6-46dd-8366-a9e99449c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = \".\"\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\"}\n",
    "    request = urllib.request.Request(url_base+file_name, headers=headers)\n",
    "    response = urllib.request.urlopen(request).read()\n",
    "    with open(file_path, mode='wb') as f:\n",
    "        f.write(response)\n",
    "\n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "\n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "\n",
    "    return data\n",
    "\n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])\n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"MNISTデータセットの読み込み\n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 画像のピクセル値を0.0~1.0に正規化する\n",
    "    one_hot_label :\n",
    "        one_hot_labelがTrueの場合、ラベルはone-hot配列として返す\n",
    "        one-hot配列とは、たとえば[0,0,1,0,0,0,0,0,0,0]のような配列\n",
    "    flatten : 画像を一次元配列に平にするかどうか\n",
    "    Returns\n",
    "    -------\n",
    "    (訓練画像, 訓練ラベル), (テスト画像, テストラベル)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "\n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "\n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
    "\n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return dataset['train_img'], dataset['train_label'], dataset['test_img'], dataset['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142c0538-e09c-47e5-8599-3c50a1ebd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa486e0-7bd2-4341-9be0-c4f194c18079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c63d77d-b68e-4254-a1e6-9931d4af1ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test .shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523d06b-e45c-4c45-9750-249f922f3c47",
   "metadata": {},
   "source": [
    "## SimpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7328747-df28-4f00-a2e6-7457e57a0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d119b24a-3665-45cd-92ae-5060db1b024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0          loss=1.0701     reg_loss=2.4579    Train_acc=0.7370     Val_acc=0.7386     lr=0.0274    \n",
      "epoch=1          loss=0.8377     reg_loss=1.9796    Train_acc=0.8362     Val_acc=0.8412     lr=0.0253    \n",
      "epoch=2          loss=1.2587     reg_loss=2.4271    Train_acc=0.8159     Val_acc=0.8175     lr=0.0234    \n",
      "epoch=3          loss=0.5936     reg_loss=1.6277    Train_acc=0.8506     Val_acc=0.8481     lr=0.0218    \n",
      "epoch=4          loss=0.9587     reg_loss=1.9636    Train_acc=0.8584     Val_acc=0.8588     lr=0.0204    \n",
      "Train Accuracy: 0.8583833333333334\n"
     ]
    }
   ],
   "source": [
    "from simplenn import Network\n",
    "from simplenn.layer import Dense\n",
    "from simplenn.activation import ReLu\n",
    "from simplenn.activation import SoftMaxLoss\n",
    "from simplenn.layer.dropout import Dropout\n",
    "from simplenn.metrics.loss import CategoricalCrossEntropy\n",
    "from simplenn.metrics import Accuracy\n",
    "from simplenn.optimizers import Adam\n",
    "\n",
    "class Model(Network):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1 = Dense(img_size, 64, W_l1=5e-4, b_l1=5e-4)\n",
    "        self.dropout1 = Dropout(rate=0.2)\n",
    "        self.activation1 = ReLu()\n",
    "        self.l2 = Dense(64, 32)\n",
    "        self.dropout2 = Dropout(rate=0.2)\n",
    "        self.activation2 = ReLu()\n",
    "        self.l3 = Dense(32, 10)\n",
    "        self.output = SoftMaxLoss(loss=CategoricalCrossEntropy())\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        # forward pass\n",
    "        x = self.l1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.l3(x)\n",
    "        return self.output(x, targets)\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.03, decay=5e-5, b1=0.9, b2=0.999)\n",
    "acc = Accuracy()\n",
    "model = Model(optimizer=optimizer)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, metrics=[acc], X_val=X_test, y_val=y_test)\n",
    "\n",
    "yprob_train = model.predict(X_train)\n",
    "train_acc = acc(yprob_train, y_train)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de5fe24-fcdf-410a-a216-f5e202840dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8588\n"
     ]
    }
   ],
   "source": [
    "yprob_test= model.predict(X_test)\n",
    "test_acc = acc(yprob_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da4d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12870f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beb691a3d7e8acd160e47ddb9ac66784b63a78ab5ee1096e93f35d22fafb9a3f"
  },
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
